import os
import json
import numpy as np
from numpy.linalg import norm
import ollama

# Get the absolute path of the chat_ollama.py file
chat_ollama_path = os.path.abspath(__file__)
# Get the directory containing the chat_ollama.py file
project_dir = os.path.dirname(chat_ollama_path)
# Set the memories directory relative to the project directory
memories_dir = os.path.join(project_dir, "..", "memories")

# Read a JSON file and return the user prompt and agent response
def read_file(filename):
    with open(os.path.join(memories_dir, filename), encoding="utf-8") as f:
        data = json.load(f)
        return data["user"], data["agent"]

def save_embeddings(filename, embeddings):
    # Create a subdirectory for the embeddings if it doesn't exist
    embeddings_dir = os.path.join(memories_dir, "embeddings")
    os.makedirs(embeddings_dir, exist_ok=True)
    # Save embeddings to a JSON file
    with open(os.path.join(embeddings_dir, f"{filename}.json"), "w") as f:
        json.dump(embeddings, f)

def load_embeddings(filename):
    # Check if the embeddings file exists in the "memories/embeddings" directory
    embeddings_file = os.path.join(memories_dir, "embeddings", f"{filename}.json")
    if not os.path.exists(embeddings_file):
        return False
    # Load embeddings from the JSON file
    with open(embeddings_file, "r") as f:
        return json.load(f)

def get_embeddings(filename, modelname):
    # Check if embeddings are already saved
    if (embeddings := load_embeddings(filename)) is not False:
        return embeddings
    # Read the user prompt and agent response from the JSON file
    user_prompt, agent_response = read_file(filename)
    # Get embeddings from ollama for the combined prompt and response
    embeddings = ollama.embeddings(model=modelname, prompt=user_prompt + "\n" + agent_response)["embedding"]
    # Save the embeddings
    save_embeddings(filename, embeddings)
    return embeddings

# Find cosine similarity of every file to a given embedding
def find_most_similar(needle, haystack):
    needle_norm = norm(needle)
    similarity_scores = [
        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack
    ]
    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)

def main(prompt):
    SYSTEM_PROMPT = """You are a helpful reading assistant who answers questions 
        based on snippets of text provided in context. Answer only using the context provided, 
        being as concise as possible. If you're unsure, just say that you don't know.
        Context:
    """
    # Get list of JSON files in the "memories" directory
    memory_files = [f for f in os.listdir(memories_dir) if f.endswith(".json")]
    embeddings = [get_embeddings(f, "nomic-embed-text") for f in memory_files]
    # Strongly recommended that all embeddings are generated by the same model (don't mix and match)
    prompt_embedding = ollama.embeddings(model="nomic-embed-text", prompt=prompt)["embedding"]
    # Find most similar files to the prompt embedding
    most_similar_files = find_most_similar(prompt_embedding, embeddings)[:5]
    context = "\n".join(read_file(memory_files[item[1]])[1] for item in most_similar_files)
    response = ollama.chat(
            model="gemma:2b",
        messages=[
            {
                "role": "system",
                "content": SYSTEM_PROMPT + context,
            },
            {"role": "user", "content": prompt},
        ],
    )
    return response["message"]["content"]
